{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: SIFT and RANSAC\n",
    "[Instruction](https://mattabrown.github.io/425/assignments/Assignment4.html)\n",
    "### Student name: Yeongu Choe\n",
    "### Student number: 77672566"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "import random\n",
    "import pickle as pkl\n",
    "from PIL import Image, ImageDraw\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path as op\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File explanation\n",
    "* main_match.py: script for matching parts\n",
    "* main_pano.py: script for making panorama image\n",
    "* hw_utils.py: helper script which should not be modified\n",
    "\n",
    "#### Terminology\n",
    "* Key point: unique point in an image that is invariant to translation, rotation and scale.\n",
    "  * Key points can be detected using SIFT algotihm.\n",
    "* Feature: Unique pattern around a key point.\n",
    "  * Feature is represented by a 128 dimensional vector, called descriptor, which is obtained by dividing the region around a key point into a grid of 4x4 sub-blocks and computing an 8-bin orientation histogram for each sub-block.\n",
    "* SIFT (Scale-Invariant Feature Transform): algorithm to find common key points in two images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: SIFT keypoint matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3: FindBestMatches()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptors1: list of normalized descriptor vector for 1st image\n",
    "# descriptors2: list of normalized descriptor vector for 2nd image\n",
    "def FindBestMatches(descriptors1: np.ndarray, descriptors2: np.ndarray, threshold: float) -> [[np.ndarray, np.ndarray]]:\n",
    "    assert isinstance(descriptors1, np.ndarray)\n",
    "    assert isinstance(descriptors2, np.ndarray)\n",
    "    assert isinstance(threshold, float)\n",
    "    # Step1: Initialize empty list\n",
    "    matched_pairs = []\n",
    "\n",
    "    # Step2: Iterate over the list of 1st descriptor vector\n",
    "    for i in range(0, descriptors1.shape[0], 1):\n",
    "        # Step 3: Initialize list of angle between two descriptor vectors\n",
    "        angle_list = []\n",
    "        # Step 4: Iterate over the list of 2nd descriptor vector\n",
    "        for j in range(0, descriptors2.shape[0], 1):\n",
    "            # Step 5: Find the cosine value between two descriptor vectors\n",
    "            # descriptor1 ⋅ descriptor2 = |descriptor1||descriptor2| cos(angle)\n",
    "            cosine_angle = np.dot(descriptors1[i], descriptors2[j])\n",
    "            # Step 6: angle between two descriptor vectors\n",
    "            angle = math.acos(cosine_angle)\n",
    "            # Step 7: append the angle into list of angles\n",
    "            angle_list.append(angle)\n",
    "        # Step 8: list the index of angle values in ascending order\n",
    "        index_list = np.argsort(angle_list)\n",
    "        # Step 9: index of the best (smallest) angle\n",
    "        best_match_index = index_list[0]\n",
    "        # Step 10: index of the second best (smallest) angle\n",
    "        second_best_match_index = index_list[1]\n",
    "        # Step 11: Best angle\n",
    "        best_angle = angle_list[best_match_index]\n",
    "        # Step 12: Second best angle\n",
    "        second_best_angle = angle_list[second_best_match_index]\n",
    "        # Step 13: calculate the ratio between two angles\n",
    "        ratio = np.abs(best_angle / second_best_angle)\n",
    "        # Step 14: if ratio is below threshold, append to mathed pair list\n",
    "        if ratio <= threshold:\n",
    "            matched_pairs.append([i, best_match_index])\n",
    "    # Step 15: return a list containing matched pairs\n",
    "    return matched_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Q1-3.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I set the threshold to be 0.63 it gave the best result. I chose 0.63 because when I set the threshold to be 0.64 it connected lines from outside of Basmati box in left image to point on right image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.4: RANSACFilter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row in keypoints: x, y, scale, orientation\n",
    "def RANSACFilter(matched_pairs: [[int, int]], keypoints1: np.ndarray, keypoints2: np.ndarray,\n",
    "                 orient_agreement_in_degree: float, scale_agreement: float) -> [[int, int]]:\n",
    "    assert isinstance(matched_pairs, list)\n",
    "    assert isinstance(keypoints1, np.ndarray)\n",
    "    assert isinstance(keypoints2, np.ndarray)\n",
    "    assert isinstance(orient_agreement_in_degree, float)\n",
    "    assert isinstance(scale_agreement, float)\n",
    "    # Step1: Initialize largest subset\n",
    "    largest_subset = []\n",
    "\n",
    "    # Step2: Iterate 10 times\n",
    "    for i in range(10):\n",
    "        # Step3: Initialize consistency_set\n",
    "        consistency_set = []\n",
    "        # Step4: Select a random pair from matched_pairs\n",
    "        selected_pair = random.choice(matched_pairs)\n",
    "\n",
    "        # Step5: Calculate orientation difference of selected pair\n",
    "        orientation1 = keypoints1[selected_pair[0]][3]\n",
    "        orientation2 = keypoints2[selected_pair[1]][3]\n",
    "        orient_difference = orientation2-orientation1\n",
    "\n",
    "        # Step6: Convert to degree by multiplying 180°/π\n",
    "        orient_difference = orient_difference*180.0/math.pi\n",
    "\n",
    "        # Step7: Calculate scale proportion of selected pair\n",
    "        scale1 = keypoints1[selected_pair[0]][2]\n",
    "        scale2 = keypoints2[selected_pair[1]][2]\n",
    "        scale_proportion = scale2/scale1\n",
    "\n",
    "        # Step8: Iterate over all pairs in matched_pairs\n",
    "        for current_pair in matched_pairs:\n",
    "            # Step9: Calculate orientation difference of pair of current iteration\n",
    "            orientation1_current_pair = keypoints1[current_pair[0]][3]\n",
    "            orientation2_current_pair = keypoints2[current_pair[1]][3]\n",
    "            current_pair_orient_difference = orientation2_current_pair-orientation1_current_pair\n",
    "            # Step10: Convert to degree by multiplying 180°/π\n",
    "            current_pair_orient_difference = current_pair_orient_difference*180.0/math.pi\n",
    "            # Step11: clip orientation difference to [0°,360°)\n",
    "            current_pair_orient_difference = current_pair_orient_difference % 360.0\n",
    "\n",
    "            # Step12: Calculate scale proportion of pair of current iteration\n",
    "            scale1_current_pair = keypoints1[current_pair[0]][2]\n",
    "            scale2_current_pair = keypoints2[current_pair[1]][2]\n",
    "            current_pair_scale_proportion = scale2_current_pair/scale1_current_pair\n",
    "\n",
    "            # Step13: Check if current pair agree with selected pair's proportion requirement\n",
    "            if (current_pair_scale_proportion < (1+scale_agreement)*scale_proportion):\n",
    "                if (current_pair_scale_proportion > (1-scale_agreement)*scale_proportion):\n",
    "                    # Step14: Check if current pair agree with selected pair's orientation requirement\n",
    "                    if (current_pair_orient_difference < orient_difference+orient_agreement_in_degree):\n",
    "                        if (current_pair_orient_difference > orient_difference-orient_agreement_in_degree):\n",
    "                            # Step15: If all condition are met, append current pair to new_set\n",
    "                            consistency_set.append(current_pair)\n",
    "\n",
    "        # Step16: Find the largest set\n",
    "        if len(consistency_set) > len(largest_subset):\n",
    "            largest_subset = consistency_set\n",
    "\n",
    "    assert isinstance(largest_subset, list)\n",
    "    return largest_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ratio threshold: 0.65, orientation agreement: 45°, scale aggrement: 30%.\n",
    "<img src=\"Q1-4.png\" width=600>\n",
    "\n",
    "#### Explanation\n",
    "Orientation consistency and scale consistency specify the acceptable difference in orientation and scale between keypoint pairs, respectively. Increasing tolerances for orientation and scale consistency allows for more matches to be considered consistent but reduces the effectiveness of filtering out false matches while increasing the chances of including true matches. Conversely, reducing tolerances filters out more false matches but may reject true matches too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Panorama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3: Keypoint projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same as transform matrix\n",
    "def KeypointProjection(xy_points: np.ndarray, h: np.ndarray) -> np.ndarray:\n",
    "    assert isinstance(xy_points, np.ndarray)\n",
    "    assert isinstance(h, np.ndarray)\n",
    "    assert xy_points.shape[1] == 2\n",
    "    assert h.shape == (3, 3)\n",
    "\n",
    "    # Step1: Transpose xy_points, so that it has (row: 2, column: N) shape\n",
    "    xy_points = xy_points.T\n",
    "    # Step2: Add a row with ones to xy_points\n",
    "    row_of_ones = np.ones((1, xy_points.shape[1]), np.float64)\n",
    "    xy_points = np.concatenate((xy_points, row_of_ones), axis=0)\n",
    "    # Step3: dot product h to xy_points\n",
    "    xy_points_out = np.dot(h, xy_points)\n",
    "    # Step4: Divide first row and second row by third row\n",
    "    xy_points_out = xy_points_out[:2]/xy_points_out[2]\n",
    "    # Step5: Transpose back to original shape (row: N, column: 2)\n",
    "    xy_points_out = xy_points_out.T\n",
    "\n",
    "    return xy_points_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Q2-3.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.4: RANSACHomography()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RANSACHomography(xy_src: np.ndarray, xy_ref: np.ndarray, num_iter: int, tol: float) -> np.ndarray:\n",
    "    assert isinstance(xy_src, np.ndarray)\n",
    "    assert isinstance(xy_ref, np.ndarray)\n",
    "    assert xy_src.shape == xy_ref.shape\n",
    "    assert xy_src.shape[1] == 2\n",
    "    assert isinstance(num_iter, int)\n",
    "    assert isinstance(tol, (int, float))\n",
    "    tol = tol*1.0\n",
    "\n",
    "    # current optimal homography matrix\n",
    "    current_best_homography_matrix = np.zeros((3, 3))\n",
    "    # number of inlier\n",
    "    maximum_number_of_inlier = 0\n",
    "\n",
    "    # Step1: iterate 'num_iter' times\n",
    "    for round in range(num_iter):\n",
    "        # Step2: randomly choose 4 matches from (x,y) list\n",
    "        rows_of_selected_pair = np.random.choice(\n",
    "            xy_src.shape[0], size=4, replace=False)\n",
    "        # Step3: compute homography matrix using cv2.findHomography()\n",
    "        homography_matrix, _ = cv2.findHomography(srcPoints=xy_src[rows_of_selected_pair],\n",
    "                                                  dstPoints=xy_ref[rows_of_selected_pair], method=cv2.RANSAC)\n",
    "        # Step4: project(transform) every (x,y) points in xy_src\n",
    "        projected_xy_src = KeypointProjection(\n",
    "            xy_points=xy_src, h=homography_matrix)\n",
    "\n",
    "        # Step5: initialize(reset) number of inlier in this 4 pairs to 0\n",
    "        number_of_inlier = 0\n",
    "\n",
    "        # Step6: calculate Euclidean distance\n",
    "        for i in range(xy_src.shape[0]):\n",
    "            # x and y from projected xy_src\n",
    "            x_projected_src = projected_xy_src[i][0]\n",
    "            y_projected_src = projected_xy_src[i][1]\n",
    "            # x and y from xy_ref\n",
    "            x_ref = xy_ref[i][0]\n",
    "            y_ref = xy_ref[i][1]\n",
    "            # Step7: calculate Euclidean distance using l2 norm\n",
    "            euclidean_distance = np.linalg.norm(\n",
    "                np.array([x_projected_src - x_ref, y_projected_src - y_ref]))\n",
    "            # Step8: count the number of inliers\n",
    "            if (euclidean_distance < tol):\n",
    "                number_of_inlier += 1\n",
    "        # Step9: if number of inliers is larger than previous best homography matrix\n",
    "        if (number_of_inlier > maximum_number_of_inlier):\n",
    "            # Step10: update highest number of inliers\n",
    "            maximum_number_of_inlier = number_of_inlier\n",
    "            # Step11: update best homography matrix\n",
    "            current_best_homography_matrix = homography_matrix\n",
    "\n",
    "    h = current_best_homography_matrix\n",
    "\n",
    "    assert isinstance(h, np.ndarray)\n",
    "    assert h.shape == (3, 3)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `tol`: 1 and `num_iter`: 10\n",
    "<img src=\"Q2-4(1,10).png\" width = 600>\n",
    "\n",
    "#### `tol`: 100 and `num_iter`: 60\n",
    "<img src=\"Q2-4(100,60).png\" width = 600>\n",
    "\n",
    "#### `tol`: 200 and `num_iter`: 10\n",
    "<img src=\"Q2-4(200,10).png\" width = 600>\n",
    "\n",
    "#### Effect of `num_iter`\n",
    "The `num_iter` parameter in RANSAC for homography estimation determines the number of iterations the algorithm performs. Increasing `num_iter` enhances robustness by exploring more random data point combinations, resulting in a more accurate homography at the cost of increased computational time. Decreasing `num_iter` speeds up the algorithm but explores fewer combinations, which can lead to less accurate homography estimates and misalignment in the panorama output.\n",
    "\n",
    "#### Effect of `tol`\n",
    "The `tol` parameter plays a crucial role in determining the quality of the output panorama in RANSAC stitching. A smaller `tol` value results in a more accurate homography estimate by enforcing stricter criteria for inliers. However, this also increases the likelihood of rejecting outliers, which can reduce the number of keypoints available for homography estimation, potentially reducing its accuracy and leading to incomplete panoramas. On the other hand, large `tol` value makes the algorithm more robust to noise and outliers by allowing for a broader acceptance range for potential inliers. However, it may also increase the risk of accepting incorrect matches, which can result in misalignment or stitching errors in the final panorama."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.5: RANSAC panorama on UBC images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fountain40.png\n",
    "<img src=\"fountain40.png\" width = 600>\n",
    "\n",
    "#### garden034.png\n",
    "<img src=\"garden034.png\" width = 600>\n",
    "\n",
    "\n",
    "#### irving_out365.png\n",
    "<img src=\"irving_out365.png\" width = 600>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
